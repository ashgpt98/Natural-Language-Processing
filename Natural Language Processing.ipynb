{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize words\n",
    "A sentence or data can be split into words using the method word_tokenize():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"All work and no play makes jack a dull boy, all work and no play\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'a', 'dull', 'boy', ',', 'all', 'work', 'and', 'no', 'play']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All work and no play makes jack dull boy.', 'All work and no play makes jack a dull boy.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing: remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'did', 'those', \"don't\", 'my', 'yourself', 'now', 'hers', 'is', 'ma', 'very', 'haven', 'couldn', 'both', 'itself', 'mightn', 'before', 'had', 'ourselves', 'then', 'further', 'having', 'has', 'most', 'by', 'hasn', 'myself', \"you'd\", 'doing', \"didn't\", 'he', 'such', 'during', 'when', 'shan', 'against', 'too', \"it's\", 'other', \"hadn't\", 'few', 'how', 'below', \"that'll\", \"she's\", 'themselves', 'just', 'are', 'ain', 'of', 'she', 'own', 'who', 'once', 'aren', 'her', 'down', 'will', 'these', 'there', 'here', 'after', 'wasn', 'nor', 'didn', 'theirs', 't', 'shouldn', 'or', \"shouldn't\", 'we', 'was', 'be', 'but', \"mustn't\", 'whom', 'only', \"shan't\", 'through', 'if', \"doesn't\", 'about', 'y', 'they', 'the', 'any', \"wasn't\", 'until', 'this', 'no', 'have', 'off', 'an', 'd', 'it', 'same', 'needn', 'won', 'each', \"should've\", \"mightn't\", 'in', \"haven't\", 'his', 'does', 'that', 'its', 's', 'wouldn', 'your', 'again', \"wouldn't\", 'weren', 'as', 'hadn', 'him', 'over', 'been', 'so', 'why', 've', 'll', 're', 'yours', 'more', 'himself', \"you'll\", 'between', 'o', 'and', 'under', 'ours', 'doesn', 'for', 'at', 'them', 'their', 'than', 'm', 'me', 'some', 'into', 'you', 'don', \"couldn't\", 'were', 'above', 'should', 'up', 'mustn', 'i', 'which', 'do', 'out', \"won't\", 'with', 'to', \"weren't\", \"you've\", 'our', 'what', 'not', 'can', 'yourselves', 'being', 'herself', 'a', 'all', 'because', \"needn't\", \"aren't\", 'while', 'from', \"hasn't\", \"isn't\", 'on', 'am', 'isn', \"you're\", 'where'}\n"
     ]
    }
   ],
   "source": [
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfilter=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    if word not in stopWords:\n",
    "        wordfilter.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', '.', 'All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', '.']\n"
     ]
    }
   ],
   "source": [
    "print(wordfilter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"game\",\"gaming\",\"gamed\",\"games\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game : game\n",
      "gaming : game\n",
      "gamed : game\n",
      "games : game\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word,':',ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gaming:game\n",
      ",:,\n",
      "the:the\n",
      "gamers:gamer\n",
      "play:play\n",
      "games:game\n"
     ]
    }
   ],
   "source": [
    "sentence = \"gaming, the gamers play games\"\n",
    "words = word_tokenize(sentence)\n",
    " \n",
    "for word in words:\n",
    "    print(word + \":\" + ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK â€“ speech tagging example\n",
    "Given a sentence or paragraph, it can label words such as verbs, nouns and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = 'Whether you\\'re new to programming or an experienced developer, it\\'s easy to learn and use Python.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Whether', 'IN'), ('you', 'PRP'), (\"'re\", 'VBP'), ('new', 'JJ'), ('to', 'TO'), ('programming', 'VBG'), ('or', 'CC'), ('an', 'DT'), ('experienced', 'JJ'), ('developer', 'NN'), (',', ','), ('it', 'PRP'), (\"'s\", 'VBZ'), ('easy', 'JJ'), ('to', 'TO'), ('learn', 'VB'), ('and', 'CC'), ('use', 'VB'), ('Python', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(document)   \n",
    "for sent in sentences:\n",
    "    print(nltk.pos_tag(nltk.word_tokenize(sent)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pythonspot.com/wp-content/uploads/2016/08/nltk-speech-codes.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter this data based on the type of word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Netherlands', 'NNP')\n",
      "('King', 'NNP')\n",
      "('Day', 'NNP')\n",
      "('San', 'NNP')\n",
      "('Francisco', 'NNP')\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import state_union\n",
    "document = 'Today the Netherlands celebrates King\\'s Day. To honor this tradition, the Dutch embassy in San Francisco invited me to'\n",
    "sentences = nltk.sent_tokenize(document)   \n",
    " \n",
    "data = []\n",
    "for sent in sentences:\n",
    "    data = data + nltk.pos_tag(nltk.word_tokenize(sent))\n",
    " \n",
    "for word in data: \n",
    "    if 'NNP' in word[1]: \n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our analysis program, we have several steps:\n",
    "\n",
    "~Data preparation\n",
    "~Feature extraction\n",
    "~Training\n",
    "~Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(word): \n",
    "    return {'last_letter': word[-1]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and training \n",
    "name = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(gender_features(n), g) for (n,g) in name] \n",
    "train_set = featuresets\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "print(classifier.classify(gender_features('James')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We start by defining 3 classes: positive, negative and neutral.\n",
    "#Each of these is defined by a vocabulary:\n",
    "positive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]\n",
    "negative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]\n",
    "neutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Every word is converted into a feature using a simplified bag of words model:\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "positive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\n",
    "negative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\n",
    "neutral_features = [(word_feats(neu), 'neu') for neu in neutral_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our training set is then the sum of these three feature sets:\n",
    "train_set = negative_features + positive_features + neutral_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "def Review(sentence):\n",
    "    neg = 0\n",
    "    pos = 0\n",
    "    sentence = sentence.lower()\n",
    "    words = sentence.split(' ')\n",
    "    for word in words:\n",
    "        classResult = classifier.classify( word_feats(word))\n",
    "    if classResult == 'neg':\n",
    "        neg = neg + 1\n",
    "    if classResult == 'pos':\n",
    "        pos = pos + 1\n",
    " \n",
    "    print('Positive: ' + str(float(pos)/len(words)))\n",
    "    print('Negative: ' + str(float(neg)/len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a='The movie was bad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 0.0\n",
      "Negative: 0.25\n"
     ]
    }
   ],
   "source": [
    "Review(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
